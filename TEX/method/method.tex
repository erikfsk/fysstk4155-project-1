\subsection{Implementation of OLS, Ridge and Lasso}

The OLS method was implemented as shown in equation \ref{eq: beta} and the Ridge
method as shown in the equation \ref{eq: ridge}. The $\hat{x}$ matrix
for both the methods, OLS and Ridge, is made as shown in the equation below:

\begin{align*}
    \hat{x} = [x^0y^0,...,x^0y^n,...,x^1y^0,x^1y^{n-1},...,x^ny^0]
\end{align*}

Scikit makes the $\hat{x}$ in a different way, but it does not matter for the result.
As shown in the next section, Implementation.
\\
\\
Lasso is a bit harder to implement, so we used scikit's version for this.
In order to understand which coefficient that corrosponded to which polynomial degree,
we compared scikits coefficient with ours from the OLS implementation. This becomes important in
the Result section.

\subsection{Implementation of MSE, R2 and $\beta$ variance}

The R2 score and MSE was implemented as shown in equation {eq: R squared} and {eq: mse}.
All the methods just simple send their predictions to a function that knows the solution.
For the $\beta$ variance it is a different story; The implementation for variance was used with the k-fold algorithm.
For each part, the expected value of $\beta$ and $\beta^2$ were added to a sum, that was then divided by
$k-1$. This was used in the equation below to obtain variance for the beta's:

\begin{align*}
    \text{VAR} = \text{E}(\beta^2) - \text{E}(\beta)^2
\end{align*}
